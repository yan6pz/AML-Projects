{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8 - Artificial Neural Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this homework, you will get your feet wet with deep learning using the PyTorch deep learning platform. This will involve:\n",
    "* Preparing data \n",
    "* Learning about the components of a deep learning pipeline\n",
    "* Setting up a model, a loss function, and an optimizer\n",
    "* Setting up training and testing loops\n",
    "* Using a visualizer like tensorboard to monitor logged data\n",
    "\n",
    "*This homework is due __April 15th 2019__. Training neural networks takes some time, particularly on CPUs so start early.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Environment\n",
    "### Working on Google Colab\n",
    "You may choose to work locally or on Google Colaboratory. You have access to free compute through this service. \n",
    "1. Visit https://colab.research.google.com/drive \n",
    "2. Navigate to the **`Upload`** tab, and upload your `HW8.ipynb`\n",
    "3. Now on the top right corner, under the `Comment` and `Share` options, you should see a `Connect` option. Once you are connected, you will have access to a VM with 12GB RAM, 50 GB disk space and a single GPU. The dropdown menu will allow you to connect to a local runtime as well.\n",
    "\n",
    "**Notes:** \n",
    "* **If you do not have a working setup for Python 3, this is your best bet. It will also save you from heavy installations like `tensorflow` if you don't want to deal with those.**\n",
    "* ***There is a downside*. You can only use this instance for a single 12-hour stretch, after which your data will be deleted, and you would have redownload all your datasets, any libraries not already on the VM, and regenerate your logs**.\n",
    "\n",
    "\n",
    "### Installing PyTorch and Dependencies\n",
    "\n",
    "The instructions for installing and setting up PyTorch can be found at https://pytorch.org/get-started/locally/. Make sure you follow the instructions for your machine. For any of the remaining libraries used in this assignment:\n",
    "* We have provided a `hw8_requirements.txt` file on the homework web page. \n",
    "* Download this file, and in the same directory you can run `pip3 install -r hw8_requirements.txt`\n",
    "\n",
    "Check that PyTorch installed correctly by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1311, 0.8062, 0.5614],\n",
       "        [0.0488, 0.9833, 0.8953],\n",
       "        [0.7917, 0.1612, 0.8353],\n",
       "        [0.0938, 0.2679, 0.4667],\n",
       "        [0.1536, 0.3520, 0.2383]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like\n",
    "\n",
    "```python\n",
    "tensor([[0.3380, 0.3845, 0.3217],\n",
    "        [0.8337, 0.9050, 0.2650],\n",
    "        [0.2979, 0.7141, 0.9069],\n",
    "        [0.1449, 0.1132, 0.1375],\n",
    "        [0.4675, 0.3947, 0.1426]])\n",
    "```\n",
    "\n",
    "### Let's get started with the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "### Part 1 -  Datasets and Dataloaders (10 points)\n",
    "\n",
    "In this section we will download the MNIST dataset using PyTorch's own API.\n",
    "\n",
    "Helpful Resources:\n",
    "* https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\n",
    "* https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "* https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "\n",
    "The `torchvision` package consists of popular datasets, model architectures, and common image transformations for computer vision. We are particularly concerned with `torchvision.datasets` and `torchvision.transforms`. Check out the API for these modules in the links provided above.\n",
    "\n",
    "**Create a directory named `hw8_data` with the following command**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir hw8_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Now use `torch.datasets.MNIST` to load the Train and Test data into `hw8_data`.** \n",
    "* ** Use the directory you created above as the `root` directory for your datasets**\n",
    "* ** Populate the `transformations` variable with any transformations you would like to perform on your data.** (Hint: You will need to do at least one)\n",
    "* **Pass your `transformations` variable to `torch.datasets.MNIST`. This allows you to perform arbitrary transformations to your data at loading time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8.19k/9.91M [00:00<02:07, 77.5kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./hw8_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9.92MB [00:02, 3.41MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./hw8_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32.8kB [00:00, 271kB/s]                            \n",
      "0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./hw8_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./hw8_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./hw8_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.65MB [00:00, 2.58MB/s]                           \n",
      "8.19kB [00:00, 111kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./hw8_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./hw8_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./hw8_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1000,), (0.3000,))\n",
    "                             ])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./hw8_data', train=True, download=True, transform=transformations) \n",
    "mnist_test = datasets.MNIST(root='./hw8_data', train=False, download=True, transform=transformations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your torch datasets have been successfully downloaded into your data directory by running the next two cells. \n",
    "\n",
    "* Each will output some metadata about your dataset. \n",
    "* Check that the training set has 60000 datapoints and a `Root Location: hw8_data`\n",
    "* Check that the testing (__also validation in our case__) set has 10000 datapoints and `Root Location: hw8_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these datasets implement the python `__len__` and `__getitem__` functions. Each element in the dataset should be a 2-tuple. What does yours look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./hw8_data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1,), std=(0.3,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mnist_train))\n",
    "print(len(mnist_train[0]))\n",
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ./hw8_data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1,), std=(0.3,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mnist_test))\n",
    "print(len(mnist_test[0]))\n",
    "mnist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any file in our dataset will now be read at runtime, and the specified transformations we need on it will be applied when we need it.**. \n",
    "\n",
    "We could iterate through these directly using a loop, but this is not idiomatic. PyTorch provides us with this abstraction in the form of `DataLoaders`. The module of interest is `torch.utils.data.DataLoader`. \n",
    "\n",
    "`DataLoader` allows us to do lots of useful things\n",
    "* Group our data into batches\n",
    "* Shuffle our data\n",
    "* Load the data in parallel using `multiprocessing` workers\n",
    "\n",
    "**Use `DataLoader` to create a loader for the training set and one for the testing set**\n",
    "* **Use a `batch_size` of 32 to start, you may change it if you wish.**\n",
    "* **Set the `shuffle` parameter to `True`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110187eb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True)\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is adapted from `show_landmarks_batch` at \n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#iterating-through-the-dataset . \n",
    "\n",
    "Run the following cell to see that your loader provides a random `batch_size` number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "%matplotlib inline\n",
    "\n",
    "def show_mnist_batch(sample_batched):\n",
    "    \"\"\"Show images for a batch of samples.\"\"\"\n",
    "    images_batch = sample_batched[0]\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.title('Batch from DataLoader')\n",
    "    \n",
    "# Displays the first batch of images\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i==1:\n",
    "        break\n",
    "    show_mnist_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2  - Models, Loss Functions and Optimizers (10 points)\n",
    "\n",
    "In this section, we will do the following:\n",
    "* Learn about how to build your deep learning model and define its parameters\n",
    "* Choose a loss function to optimize\n",
    "* Choose an optimization method to maximize/minimize the loss\n",
    "\n",
    "We'll first start with a single layer neural network to do handwritten digit classification. The math may ring some bells from homework 7.\n",
    "\n",
    "`torch.nn` is the module we will be using here. You can find the API at https://pytorch.org/docs/stable/nn.html. There is also a quick summary at https://pytorch.org/tutorials/beginner/nn_tutorial.html#closing_thoughts.\n",
    "\n",
    "#### Models\n",
    "\n",
    "We will use the following python modules in building our one layer model.\n",
    "\n",
    "* `torch.nn.Module`: Your model will be abstracted as a python class. Your python class must subclass `torch.nn.Module`. It is the base class for all neural network modules in PyTorch (Do not confuse python modules with PyTorch Modules). These implement the `forward()` function which defines how your model handles input and produces an output. Your model class can also have `torch.nn.Module`s as members, allowing nested tree like structures, and it is leveraging this that you are able to build neural networks in PyTorch.   \n",
    "\n",
    "* `torch.nn.Linear`: A unit of computation in neural networks are *Layers* and PyTorch provides abstractions for layers as `nn.Modules`. These come in many forms including *Convolutional*, *Recurrent*, and *Linear*. You can find the API for linear layers here https://pytorch.org/docs/stable/nn.html#linear-layers.\n",
    "\n",
    "**Now use the information provided to define the `OneLayerModel` class below. The superclass constructor has been called for you, and this allows your subclass to access superclass methods and members.**\n",
    "* **Finish the `__init__()` function.**\n",
    "* **Finish the `forward()` function.** (Hint: Use that fact that layer modules implement their own `forward()` function)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OneLayerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.flin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = self.flin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions and Optimizers\n",
    "\n",
    "You've defined your model but now what? It's just a black box that takes an input and spits out some numbers. You haven't yet defined what it means to be a good or bad model. \n",
    "\n",
    "A ***Loss Function*** takes what your model outputs and compares it to what it *should* have put out. It returns some  meaningful value used to update your model parameters, and so train your model. Check out Section 21.2.1 of the textbook for more details about types of loss functions. The Loss function represents the overall goal of building this model, and the choice of loss function is very important. \n",
    "\n",
    "We must examine our model parameters and our problem instance to see about how to choose a loss function.\n",
    "* We take in a 784-dimensional vector and output 10 real values, giving our model 784 x 10 parameters. \n",
    "* It is natural given that our problem is an instance of *multi-class classification* that we would want each of our output values to model `P(y==i|x)`.\n",
    "* If we go this route, we get an added constraint that the sum of all 10 of our output values should be 1 (forming a probability mass distribution).\n",
    "\n",
    "Turns out there is a very convenient loss function for just our use case known as ***cross-entropy loss***. Check out this reference https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy for a little more intuition on this.\n",
    "\n",
    "Once again, PyTorch has abstractions built in for us in the `torch.nn` module, namely `torch.nn.CrossEntropyLoss`. The API can be found at https://pytorch.org/docs/stable/nn.html#crossentropyloss. \n",
    "\n",
    "We're still not ready to train our model because while we have some parameters, and we have some measure of how good or bad our predictions are, we have no notion of how to go about updating our parameters in order to improve our loss. \n",
    "\n",
    "This is where ***Optimizers*** come in. In general, we have one main way of minimizing loss functions (training our models), and that is through *Stochastic Gradient Descent* https://en.wikipedia.org/wiki/Stochastic_gradient_descent. There are many variants and optimizations of this method, however, and the `torch.optim` package gives us abstractions for these. The API can be found at https://pytorch.org/docs/stable/optim.html#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Training and Validation (45 points)\n",
    "\n",
    "In this section we will learn how to use the concepts we've learned about so far to train the model we built, and validate how well it does.We also want to monitor how well our training is going while it is happening. \n",
    "\n",
    "For this we can use a package called `tensorboardX`. You will need to install this package using `pip` or `Anaconda`, based on your dev environment. Additionally, we'll want to use a logging module called `tensorboardX.SummaryWriter`. You can consult the API here https://tensorboardx.readthedocs.io/en/latest/tutorial.html. Run the next cell to ensure that all is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Try uncommenting these commands if you're facing issues here\n",
    "!pip3 install -U protobuf\n",
    "!pip3 install -U tensorflow\n",
    "!pip3 install -U tensorboardX\n",
    "\"\"\"\n",
    "%load_ext tensorboard.notebook\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided the code to use `tensorboard` just before calling your `train` function. You don't have to change the top-level log directory, but you can create multiple runs (different parameters or versions of your code) just by creating subdirectories for these within your top-level directory.\n",
    "\n",
    "**Now use the information provided above to do the following:**\n",
    "* ** Instantiate a `OneLayerModel` with the appropriate input/output parameters.**\n",
    "* ** Define a cross-entropy loss function.**\n",
    "* ** Define a stochastic gradient descent optimizer based for you model's parameters. Start with a learning rate of 0.001, and adjust as necessary. You can start with the vanilla `optim.SGD` optimizer, and change it if you wish.** \n",
    "* **Create a `SummaryWriter` object that will be responsible for logging our training progress into a directory called `logs/expt1` (Or whatever you wish your top-level directory to be called).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "model = OneLayerModel(1*28*28, 10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "writer = SummaryWriter('logs/expt1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've finally come to the point where we need to write our training set up. We're going to use both our training and testing (validation) sets for this. Note that traditionally, you would separate part of your training data into validation data in order to get an unbiased estimate of how your model performs, but here we'll just pretend that our testing data is our validation data. \n",
    "\n",
    "**Training a model with batches of data broadly involves the following steps:**\n",
    "1. **One `epoch` is defined as a full pass of your dataset through your model. We choose the number of epochs we wish to train our model for.**\n",
    "2. **In each epoch, set your model to train mode.** \n",
    "3. **you feed your model `batch_size` examples at a time, and receive `batch_size` number of outputs until you've gotten through your entire dataset.**\n",
    "4. **Calculate the loss function for those outputs given the labels for that batch.**\n",
    "5. **Now calculate the gradients for each model parameter.** (Hint: Your loss function object can do this for you)\n",
    "6. **Update your model parameters** (Hint: The optimizer comes in here)\n",
    "7. **Set the gradients in your model to zero for the next batch.**\n",
    "8. **After each epoch, set your model to evaluation mode.**\n",
    "9. **Now evaluate your model on the validation data. Log the total loss and accuracy over the validation data.** (Note: PyTorch does automatic gradient calculations in the background through its `Autograd` mechanism https://pytorch.org/docs/stable/notes/autograd.html. Make sure to do evaluation in a context where this is turned off!)\n",
    "\n",
    "**Complete the `train()` function below. Try to make it as general as possible, so that it can be used for improved versions of you model. Feel free to define as many helper functions as needed.**\n",
    "**Make sure that you do the following: **\n",
    "* **Log the *training loss* and *training accuracy* on each batch for every epoch, such that it will show up on `tensorboard`.**\n",
    "* **Log the loss on the validation set and the accuracy on the validation set every epoch**\n",
    "\n",
    "**You will need to produce the plots for these.**\n",
    "\n",
    "You may also want to add some print statements in your training function to report progress in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, val_loader, loss_func, optimizer,num_epochs=10, writer=None):\n",
    "    test(model, val_loader, loss_func, 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_internal(model, train_loader, loss_func, optimizer, writer, epoch)\n",
    "        test(model, val_loader, loss_func, epoch)\n",
    "\n",
    "        \n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "log_interval = 100\n",
    "\n",
    "def train_internal(model, train_loader, loss_func, optimizer, writer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_item = loss\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss_item))\n",
    "            \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        accuracy = 100. * correct / len(train_loader.dataset)\n",
    "        train_losses.append(loss_item)\n",
    "        train_accuracies.append(accuracy)\n",
    "        \n",
    "        writer.add_scalar('train_losses',loss_item, batch_idx)\n",
    "        writer.add_scalar('train_accuracies',accuracy, batch_idx)\n",
    "            \n",
    "            \n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "def test(model, val_loader, loss_func, epoch_num):\n",
    "    model.eval()\n",
    "    loss_item = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss_item += loss_func(output, target)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "    loss_item /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    test_losses.append(loss_item)\n",
    "    test_accuracies.append(accuracy)\n",
    "    writer.add_scalar('test_losses',loss_item, epoch_num)\n",
    "    writer.add_scalar('test_accuracies',accuracy, epoch_num)\n",
    "    \n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    loss_item, correct, len(val_loader.dataset),accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally call `train` with the relevant parameters. Run the tensorboard command on your top-level logs directory to monitor training. If there is logging data from a previous run, just delete the directory for the run, and reinstantiate the `SummaryWriter` for that run. (You may want to reinstantiate the model itself if you want to clear the model parameters too).\n",
    "\n",
    "Note : This function may take a while to complete if you're training for many epochs on a cpu. This is where it comes in handy to be running on Google Colab, or just have a GPU on hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "\n",
      "Test set: Avg. loss: 0.0732, Accuracy: 1466/10000 (14.00%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.378837\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.229748\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.345106\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.544333\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.231431\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.268759\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.450474\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.204893\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.201746\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.592271\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.278198\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.196191\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.259013\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.377685\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.151320\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.150975\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.492140\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.490384\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.284158\n",
      "\n",
      "Test set: Avg. loss: 0.0092, Accuracy: 9174/10000 (91.00%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.342952\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.452045\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.380040\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.182970\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.520432\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.618846\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.191036\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.345279\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.261031\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.331075\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.620639\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.221974\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.375326\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.245435\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.151924\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.140522\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.286016\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.199044\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.238330\n",
      "\n",
      "Test set: Avg. loss: 0.0092, Accuracy: 9159/10000 (91.00%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.345797\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.194577\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.243967\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.333160\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.137283\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.121438\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.046319\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.133947\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.179742\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.361322\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.397949\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.156411\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.236817\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.067092\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.587909\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.519952\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.127790\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.415536\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.302942\n",
      "\n",
      "Test set: Avg. loss: 0.0088, Accuracy: 9189/10000 (91.00%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.378928\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.058662\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.223280\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.135156\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.306029\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.367646\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.215012\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.089139\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.157926\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.544409\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.314325\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.183577\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.370589\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.397968\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.205971\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.287160\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.233703\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.172079\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.206709\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9214/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.327206\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.209627\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.097306\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.215018\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.267901\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.254542\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.145164\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.218633\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.426452\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.444917\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.846789\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.236775\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.151206\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.406844\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.516600\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.366854\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.403449\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.118399\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.161884\n",
      "\n",
      "Test set: Avg. loss: 0.0090, Accuracy: 9176/10000 (91.00%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.140652\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.188915\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.198083\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.316846\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.090498\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.123959\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.247618\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.570031\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.078814\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.132278\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.264357\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.204397\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.361772\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.465185\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.368606\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.121309\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.199613\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.290375\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.148775\n",
      "\n",
      "Test set: Avg. loss: 0.0086, Accuracy: 9225/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.081379\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.238412\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.133770\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.210871\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.205365\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.261531\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.177226\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.445673\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.231247\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.400641\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.279653\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.287619\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.095296\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.407383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.716256\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.503291\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.163787\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.143233\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.293080\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9232/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.139273\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.317434\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.068652\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.183425\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.366791\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.058643\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.324975\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.332239\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.393645\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.114008\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.484580\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.113209\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.680047\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.250166\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.430978\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.207123\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.421292\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.392195\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.240458\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9232/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.361323\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.346226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.092370\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.465259\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.144628\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.362560\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.318411\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.364437\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.274792\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.482443\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.060519\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.348227\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.353637\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.099859\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.314178\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.331163\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.247669\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.174472\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.323184\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9222/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.048763\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.411129\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.229855\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.479542\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.105333\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.047621\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.460128\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.599216\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.259449\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.176347\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.151133\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.357827\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.149985\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.294026\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.287887\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.388058\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.565914\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.363839\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.127603\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9226/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.232357\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.048574\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.134614\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.281579\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.154061\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.048504\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.128633\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.294014\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.172442\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.077610\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.243303\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.227211\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.231778\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.193914\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.461868\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.073784\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.376326\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.211235\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.072084\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9225/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.165097\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.295814\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.174854\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.350121\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.289274\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.336741\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.194190\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.417569\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.191367\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.152237\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.139890\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.115370\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.421346\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.148634\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.474150\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.222808\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.341697\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.132271\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.136575\n",
      "\n",
      "Test set: Avg. loss: 0.0086, Accuracy: 9242/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.168077\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.346859\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.154733\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.184427\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.206170\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.079895\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.366858\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.728903\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.214290\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.275932\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.194861\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.166747\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.110796\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.101786\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.255372\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.372558\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.302522\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.317821\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.504070\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9229/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.137979\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.167828\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.180826\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.108824\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.221908\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.039931\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.062532\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.331692\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.203134\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.735353\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.240180\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.223234\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.395924\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.232626\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.368241\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.435911\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.414149\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.142398\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.280351\n",
      "\n",
      "Test set: Avg. loss: 0.0086, Accuracy: 9258/10000 (92.00%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.318602\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 0.136275\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.057248\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 0.542147\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.164376\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.328955\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.268166\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 0.154006\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.136001\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 0.145194\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.122467\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 0.138830\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.213017\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 0.074096\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.265024\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.468292\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.144840\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 0.333427\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.208852\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 9227/10000 (92.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%tensorboard --logdir=logs\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "\n",
    "train(model, train_loader, test_loader, criterion, optimizer, 15, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final Validation Loss:__ *0.0086*\n",
    "\n",
    "__Final Validation Accuracy:__ *92%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is familiar about a 1-layer neural network with cross-entopy loss? Have you seen this before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Two Layer Neural Net (20 points)\n",
    "\n",
    "The thing that makes neural networks really powerful is that they are able to do complex function approximation. As we saw earlier, we can organize the computation done in neural networks into units called *layers*. In a general neural network, there is an *input layer*, and an *output layer*. These may be the same layer as they were in our previous example. When they are not the same, there are intermediate layers known as _hidden layers_. These layers receive input from other layers and send their output to other layers. \n",
    "\n",
    "We have been dealing with a certain type of neural network known as a __fully connected__ network. For our purposes, this just means that the output of the layer is just the dot product of its input `x`, its weights `w` plus a bias term `b`, all wrapped in a non-linear *activation function* `F`.  \n",
    "\n",
    "`y = F(w^T x + b)`.\n",
    "\n",
    "These non-linear activation functions are very important but where in our last neural network did we apply such a function? Implicitly we applied what's known as a __softmax activation__ in order to compute cross-entropy loss https://en.wikipedia.org/wiki/Softmax_function.\n",
    "\n",
    "We'll now try to create a neural network with one hidden layer. This means that we have to come up with an activation function for the output of that hidden layer. A famous, simple but powerful activation function is the __Rectified Linear Unit (ReLU)__ function defined nas `ReLU(x) = max(x,0)`. We will use this on the output of the hidden layer.\n",
    "\n",
    "`torch.nn` has a module known as `nn.Sequential` that allows us to chain together other modules. This module implements a `forward()` function that automatically handles input-output connections etc. Check out the API at https://pytorch.org/docs/stable/nn.html#sequential. \n",
    "\n",
    "**Just like you did with the single layer model, define a class `TwoLayerModel`, a neural network with ReLU activation for the hidden layer. `nn.Sequential` may come in handy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers):\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.flin1 = nn.Linear(input_dim, 50)\n",
    "        self.flin2 = nn.Linear(50, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = self.relu(self.flin1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.flin2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once again use the information provided above to do the following:**\n",
    "* ** Instantiate a `TwoLayerModel` with the appropriate input/output/hidden layer parameters.**\n",
    "* ** Define a cross-entropy loss function again.**\n",
    "* ** Define a stochastic gradient descent optimizer based for you model's parameters. Start with a learning rate of 0.001, and adjust as necessary. You can start with the vanilla `optim.SGD` optimizer, and change it if you wish.** \n",
    "* **Create a `SummaryWriter` object that will be responsible for logging our training progress into a directory called `logs/expt2` (Or whatever you wish your top-level directory to be called, just make sure the subdirectory is different from your previous SummaryWriter).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = TwoLayerModel(12*12*20, 10, 2)\n",
    "learning_rate=0.01\n",
    "# Loss and optimizer\n",
    "loss2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "writer2 = SummaryWriter('logs/expt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `train` on your two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "\n",
      "Test set: Avg. loss: 0.0047, Accuracy: 1062/10000 (10.00%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.372935\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.670982\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.481474\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.177920\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.578010\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.529041\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.420279\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.321304\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.225186\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.367543\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162648\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.559480\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.218679\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.394042\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.293793\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.313540\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.291788\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.259643\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.072840\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9683/10000 (96.00%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.180220\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.058014\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.358479\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.101715\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.245543\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.218397\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.138459\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.060675\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.129615\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.081836\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.022201\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.384495\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.195452\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.052254\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.321011\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.074105\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.292264\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.116758\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.176325\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 9771/10000 (97.00%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.495691\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.124591\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.136104\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.066559\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.123954\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.014972\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.355121\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.183686\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.175932\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.127193\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.029371\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.177940\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.053290\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.063084\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.209875\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.038826\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.090481\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.276834\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.156587\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 9807/10000 (98.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%tensorboard --logdir=logs\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                           batch_size=500, \n",
    "                                           shuffle=True)\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "\n",
    "train(model2, train_loader, test_loader, loss2, optimizer2, 3, writer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final Validation Loss:__ *0.0001*\n",
    "\n",
    "__Final Validation Accuracy:__ *98%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did your accuracy on the validation set improve with multiple layers? Why do you think this is ?\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - What is being learned at each layer? (10 points)\n",
    "\n",
    "So what exactly are these weights that our network is learning at each layer? By conveniently picking our layer dimensions as perfect square numbers, we can try to visualize the weights learned at each layer as square images. Use the following function to do so for *all interesting layers* across your models. Feel free to modify the function as you wish. \n",
    "\n",
    "**At the very least, you must generate:**\n",
    "1. **The ten 28x28 weight images learned by your one layer model.**\n",
    "2. **The 256 28x28 weight images learned by the hidden layer in your two-layer model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_weights(model, layer_idx, num_images, image_dim, title):\n",
    "    # Find number of rows and columns based on number of images\n",
    "    for d in range(1,num_images):\n",
    "        f = num_images/d\n",
    "        if int(f)==f:\n",
    "            dim1 = int(min(f,d))\n",
    "            dim2 = int(max(f,d))\n",
    "        if d > f:\n",
    "            break    \n",
    "    # Plot weights as square images\n",
    "    fig, ax  = plt.subplots(dim1, dim2)\n",
    "    \n",
    "    # At least 1 inch by 1 inch images\n",
    "    fig.set_size_inches(dim2, dim1)\n",
    "    weights = (list(model.parameters())[layer_idx])\n",
    "    fig.suptitle(title)\n",
    "    for i in range(dim1):\n",
    "        for j in range(dim2):\n",
    "            ax[i][j].imshow(weights[dim2*i+j].reshape(image_dim,image_dim).detach().numpy(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
